---
layout: page
title: About
permalink: /about/
---
Why does the technology world have a cybersecurity problem? If you believe that "software is eating the world", and maybe also that software has already eaten the world, then perhaps much of the answer lies in how we develop software.

In particular, to the extent that each existing software vulnerability is a security bug that was unwittingly written into code by a developer, can we prevent new bugs from being introduced while writing software? After all, if the well-meaning, but busy developer knows how to write secure code, would they still otherwise choose to write vulnerable code?

Perhaps this is possible with some thoughtful combination of training, tooling, and process. 

At least that is the hypothesis I put to test daily as an Application Security engineer, where I try to help teams by implementing self-service infrastructure that (ideally) provides developers with alert data at the "right time", in phase with their SDLC, when bugs are cheapest to fix.

However, what I describe primarily leverages SAST (static application security testing) at the pull request, which is great for what it is, as a timely safety net to catch known security bugs that we can detect through static analysis of code, but are there meaningful things we can do earlier in the SDLC -- especially to avoid writing the bug in first place?

 -- JW
